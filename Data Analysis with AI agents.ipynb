{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import json\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from io import StringIO\n",
    "import plotly.express as px\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "def app():\n",
    "    if \"history\" not in st.session_state:\n",
    "        st.session_state.history = []\n",
    "\n",
    "    if \"user_history\" not in st.session_state:\n",
    "        st.session_state.user_history = \"\"\n",
    "\n",
    "    st.title(\"Welcome to the Insto Querying Engine\")\n",
    "    st.markdown(\"A Querying Engine for Institutional Data. Inputs user query, outputs data based from Hamilton nodes\")\n",
    "\n",
    "    st.divider()\n",
    "\n",
    "    models = [\"gemini-2.5-pro\", \"gpt-4o\", \"claude-sonnet-4\", \"gemini-2.5-flash\", \"gpt-4o-mini\", \"claude-3-5-haiku\", \"claude-3-5-sonnet\", \"claude-3-5-sonnet-v2\", \"claude-3-7-sonnet\", \"claude-3-haiku\", \"claude-opus-4\", \"dall-e-3\", \"gemini-2.0-flash-001\", \"gemini-2.0-flash-lite-001\", \"gpt-image-1\", \"o1\", \"o1-mini\", \"o3\", \"o3-mini\", \"o4-mini\", \"self_hosted_llama33_70\", \"text-embedding-3-large\", \"text-embedding-3-small\", \"text-embedding-ada-002\"]\n",
    "\n",
    "    # Create a select box for the models\n",
    "    if \"selected_model\" not in st.session_state:\n",
    "        st.session_state.selected_model = models[0]\n",
    "    st.session_state.selected_model = st.sidebar.selectbox(\"Select OpenAI model\", models, index=models.index(st.session_state.selected_model))\n",
    "\n",
    "    agent = Agent(\n",
    "    model=OpenAIModel(st.session_state.selected_model, provider=OpenAIProvider(api_key=os.environ['CDP_CLIENT_API_KEY'], base_url=base_url)),\n",
    "    output_type=NodeAttributes,\n",
    "    system_prompt=get_prompt\n",
    "    )\n",
    "\n",
    "    agent_aggregator = Agent(\n",
    "        model=OpenAIModel(st.session_state.selected_model, provider=OpenAIProvider(api_key=os.environ['CDP_CLIENT_API_KEY'], base_url=base_url)),\n",
    "        model_settings={'temperature': 0.0, 'seed': 42},\n",
    "        output_type=[AggregationPlan],\n",
    "        system_prompt=\"\"\"Generate a step by step plan as list.\"\"\"  # we'll overwrite this later with the correct system prompt\n",
    "    )\n",
    "\n",
    "    agent_graphs = Agent(\n",
    "        model=OpenAIModel(st.session_state.selected_model, provider=OpenAIProvider(api_key=os.environ['CDP_CLIENT_API_KEY'], base_url=base_url)),\n",
    "        model_settings={'temperature': 0.0, 'seed': 42},\n",
    "        output_type=ChartRecommendation,\n",
    "        system_prompt=\"\"\"Generate a step by step plan as list.\"\"\"  # we'll overwrite this later with the correct system prompt\n",
    "    )\n",
    "\n",
    "    agent_router = Agent(\n",
    "        model=OpenAIModel(st.session_state.selected_model, provider=OpenAIProvider(api_key=os.environ['CDP_CLIENT_API_KEY'], base_url=base_url)),\n",
    "        model_settings={'temperature': 0.0, 'seed': 42},\n",
    "        output_type=QueryAction,\n",
    "        system_prompt=router_prompt\n",
    "    )\n",
    "\n",
    "    agent_else = Agent(\n",
    "        model=OpenAIModel(st.session_state.selected_model, provider=OpenAIProvider(api_key=os.environ['CDP_CLIENT_API_KEY'], base_url=base_url)),\n",
    "        model_settings={'temperature': 0.0, 'seed': 42},\n",
    "        system_prompt=\"\"\"Take the user's query and provide a response. If the user's query is not related to data, prompt the user in a friendly and professional manner.'\"\"\"\n",
    "    )\n",
    "\n",
    "    # with st.container():\n",
    "    st.markdown('<div class=\"sticky-container\">', unsafe_allow_html=True)\n",
    "\n",
    "    # Business Area Browser Section (moved to main page)\n",
    "    st.caption(\"Select business area:\")\n",
    "\n",
    "    if \"selected_business_area\" not in st.session_state:\n",
    "        st.session_state.selected_business_area = None\n",
    "\n",
    "    business_areas = list(im.feature_descriptions.keys())\n",
    "\n",
    "    # Create buttons for business areas in a more compact grid layout\n",
    "    cols = st.columns(6)  # 6 columns for more compact layout\n",
    "\n",
    "    for i, business_area in enumerate(business_areas):\n",
    "        with cols[i % 6]:\n",
    "            # Check if this business area is currently selected\n",
    "            is_selected = st.session_state.get('selected_business_area') == business_area\n",
    "            button_type = \"primary\" if is_selected else \"secondary\"\n",
    "           \n",
    "            if st.button(business_area, key=f\"business_{i}\", type=button_type):\n",
    "                st.session_state.selected_business_area = business_area\n",
    "                st.rerun()\n",
    "   \n",
    "    st.divider()\n",
    "\n",
    "    # Initialize messages if not exists\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    if \"feature_name\" not in st.session_state:\n",
    "        st.session_state.feature_name = None\n",
    "\n",
    "    # Display chat messages from history on app rerun\n",
    "    # for message in st.session_state.messages:\n",
    "    #     with st.chat_message(message[\"role\"]):\n",
    "    #         st.markdown(message[\"content\"])\n",
    "\n",
    "    default = \"\"\n",
    "    if st.session_state.selected_business_area:\n",
    "        default = defaults[st.session_state.selected_business_area]\n",
    "\n",
    "\n",
    "    # Accept user input\n",
    "    if prompt := st.chat_input(default):\n",
    "        if st.session_state.selected_business_area == None:\n",
    "            st.error(\"Please select a business area first! Choose an area from the buttons above.\")\n",
    "            return\n",
    "       \n",
    "        else: # Add user message to chat history\n",
    "            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            # Display user message in chat message container\n",
    "            with st.chat_message(\"user\"):\n",
    "                st.markdown(prompt)\n",
    "\n",
    "            # Display assistant response in chat message container\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                try:\n",
    "                    # Build conversation context for the agent\n",
    "                    conversation_context = \"\\n\".join([\n",
    "                        f\"{msg['role']}: {msg['content']}\"\n",
    "                        for msg in st.session_state.messages[:-1]  # Exclude the current user message\n",
    "                    ])\n",
    "\n",
    "                    if len(st.session_state.messages) > 1:\n",
    "                        st.session_state.user_history = st.session_state.messages[-2]['content']\n",
    "                    else:\n",
    "                        st.session_state.user_history = \"\"\n",
    "\n",
    "                    # Get selected business area if one exists, otherwise use None\n",
    "                    selected_area = st.session_state.get('selected_business_area', None)\n",
    "                    # Prepare the full prompt with context\n",
    "                    user_query = f\"{conversation_context}\\nuser: {prompt}\" if conversation_context else prompt\n",
    "                    user_input = user_query + \". `business_area` is \" + selected_area\n",
    "\n",
    "                    start_date, end_date, feature_name, business, domain, granularity, hamilton_description, source_file, node_match = im.get_query_attributes(user_input, agent)\n",
    "                    if feature_name in feature_aggregations:\n",
    "                        agg_func = feature_aggregations[st.session_state.selected_business_area][feature_name]\n",
    "                    else:\n",
    "                        agg_func = \"sum\"\n",
    "                    st.session_state.node_description = hamilton_description\n",
    "                   \n",
    "                    # Get response from Pydantic AI agent\n",
    "                    with st.spinner(\"Thinking...\"):\n",
    "                        if \"active_df\" not in st.session_state:\n",
    "\n",
    "                            # result = agent.run_sync(full_prompt)\n",
    "                            response = im.get_feature_output(feature_name, start_date, end_date)\n",
    "                            df = response.copy()\n",
    "                            result = im.groupby_aggregation_from_text(df, \"ds\", feature_name, agg_func)\n",
    "                            result = im.format_large_number(result)\n",
    "                            if agg_func == \"last\":\n",
    "                                st.markdown(f'###### The {agg_func} {feature_name} from {start_date} to {end_date} was {result}. ')\n",
    "                            else:\n",
    "                                st.markdown(f'###### The {agg_func} of {feature_name} from {start_date} to {end_date} was {result}. ')\n",
    "\n",
    "                            st.subheader(f\"Sample Data for: {feature_name}\")\n",
    "                            df_old, df = im.aggregate_df(df, agg_func, feature_name)\n",
    "                            st.dataframe(df_old, hide_index=True)\n",
    "\n",
    "                            st.session_state.active_df = df_old.copy()\n",
    "                            st.session_state.feature_name = feature_name\n",
    "\n",
    "                            df_download = st.session_state.active_df\n",
    "                            # Convert DataFrame to CSV\n",
    "                            csv_buffer = StringIO()\n",
    "                            df_download.to_csv(csv_buffer, index=False)\n",
    "                            csv_data = csv_buffer.getvalue()\n",
    "                            # Display download button\n",
    "                            st.download_button(\n",
    "                                label=\"Download as CSV\",\n",
    "                                data=csv_data,\n",
    "                                file_name=\"active_data.csv\",\n",
    "                                mime=\"text/csv\"\n",
    "                            )\n",
    "\n",
    "                            st.markdown(f'###### Please let me know if I can filter or aggregate the data differently! Would you like me to plot the data?')\n",
    "                            st.session_state.history.append((st.session_state.user_history, df_download.copy(), result.__str__()))\n",
    "\n",
    "                        else:\n",
    "                            action = agent_router.run_sync(user_input).output.action\n",
    "\n",
    "                            if action == 'get_data':\n",
    "                                # result = agent.run_sync(full_prompt)\n",
    "                                response = im.get_feature_output(feature_name, start_date, end_date)\n",
    "                                df = response.copy()\n",
    "                                result = im.groupby_aggregation_from_text(df, \"ds\", feature_name, agg_func)\n",
    "                                result = im.format_large_number(result)\n",
    "                                if agg_func == \"last\":\n",
    "                                    st.markdown(f'###### The {agg_func} {feature_name} from {start_date} to {end_date} was {result}.')\n",
    "                                else:\n",
    "                                    st.markdown(f'###### The {agg_func} of {feature_name} from {start_date} to {end_date} was {result}.')\n",
    "\n",
    "                                st.subheader(f\"Sample Data for: {feature_name}\")\n",
    "                                df_old, df = im.aggregate_df(df, agg_func, feature_name)\n",
    "                                st.dataframe(df_old, hide_index=True)\n",
    "\n",
    "                                st.session_state.active_df = df_old.copy()\n",
    "                                st.session_state.feature_name = feature_name\n",
    "\n",
    "                                df_download = st.session_state.active_df\n",
    "                                # Convert DataFrame to CSV\n",
    "                                csv_buffer = StringIO()\n",
    "                                df_download.to_csv(csv_buffer, index=False)\n",
    "                                csv_data = csv_buffer.getvalue()\n",
    "                                # Display download button\n",
    "                                st.download_button(\n",
    "                                    label=\"Download as CSV\",\n",
    "                                    data=csv_data,\n",
    "                                    file_name=\"active_data.csv\",\n",
    "                                    mime=\"text/csv\"\n",
    "                                )\n",
    "\n",
    "                                st.session_state.history.append((st.session_state.user_history, df_download.copy(), result.__str__()))\n",
    "\n",
    "                            elif action == 'aggregate_data':\n",
    "                                agent_aggregator._system_prompts = (generate_system_prompt(st.session_state.active_df))\n",
    "                                result = agent_aggregator.run_sync(user_input)\n",
    "                                if hasattr(result, \"output\"):\n",
    "                                    kwargs = result.output.__dict__\n",
    "                                    st.write(result.__str__())\n",
    "                                    st.write(kwargs.get('sql', ''))\n",
    "                                    func_name = kwargs.pop('function')\n",
    "                                    try:\n",
    "                                        new_df = SAFE_TOOLS[func_name](st.session_state.active_df, **kwargs)\n",
    "                                        st.session_state.history.append((st.session_state.user_history, new_df.copy(), result.__str__()))\n",
    "                                        st.session_state.active_df = new_df.copy()\n",
    "                                        st.success(\"Applied transformation\")\n",
    "                                    except Exception as e:\n",
    "                                        st.error(f\"Could not request - do nothing!\\nerror_message={str(e)}\")\n",
    "\n",
    "                                st.session_state.feature_name = feature_name\n",
    "                                st.dataframe(st.session_state.active_df, hide_index=True)\n",
    "\n",
    "                                df_download = st.session_state.active_df\n",
    "                                # Convert DataFrame to CSV\n",
    "                                csv_buffer = StringIO()\n",
    "                                df_download.to_csv(csv_buffer, index=False)\n",
    "                                csv_data = csv_buffer.getvalue()\n",
    "                                # Display download button\n",
    "                                st.download_button(\n",
    "                                    label=\"Download as CSV\",\n",
    "                                    data=csv_data,\n",
    "                                    file_name=\"active_data.csv\",\n",
    "                                    mime=\"text/csv\"\n",
    "                                )\n",
    "\n",
    "                            elif action == 'graph_data':\n",
    "                                df_download = st.session_state.active_df\n",
    "                                # Convert DataFrame to CSV\n",
    "                                csv_buffer = StringIO()\n",
    "                                df_download.to_csv(csv_buffer, index=False)\n",
    "                                csv_data = csv_buffer.getvalue()\n",
    "                                # Display download button\n",
    "                                st.download_button(\n",
    "                                    label=\"Download as CSV\",\n",
    "                                    data=csv_data,\n",
    "                                    file_name=\"active_data.csv\",\n",
    "                                    mime=\"text/csv\"\n",
    "                                )\n",
    "\n",
    "                                agent_graphs._system_prompts = (generate_graphing_system_prompt(st.session_state.active_df, context=st.session_state.node_description))\n",
    "                                result = agent_graphs.run_sync(user_input)\n",
    "                                if hasattr(result, \"output\"):\n",
    "                                    # st.write(result.__str__())\n",
    "                                    try:\n",
    "                                        graph = im.plot_with_agent_recommendation(st.session_state.active_df, result.output)\n",
    "                                        st.session_state.history.append((st.session_state.user_history, st.session_state.active_df.copy(), result.__str__()))\n",
    "                                        # st.session_state.active_df = new_df.copy()\n",
    "                                        st.success(\"Plotted successfully\")\n",
    "                                    except Exception as e:\n",
    "                                        st.error(f\"Could not request - do nothing!\\nerror_message={str(e)}\")\n",
    "\n",
    "                                st.session_state.feature_name = feature_name\n",
    "                                st.plotly_chart(graph, use_container_width=True)\n",
    "\n",
    "                            elif action == 'outlier_analysis':\n",
    "                                df_download = st.session_state.active_df.copy()\n",
    "                                ad_result = anomalyDetection(df_download)\n",
    "                                if not ad_result:\n",
    "                                    st.write(\"No outliers detected\")\n",
    "                                else:\n",
    "                                    for i in ad_result:\n",
    "                                        fig = px.line(ad_result[i], x=list(ad_result[i][ad_result[i].columns[0]]), y=list(ad_result[i][ad_result[i].columns[-1]]), title=f\"Outlier Analysis for {i}\")\n",
    "                                        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "                            else:\n",
    "                                df_download = st.session_state.active_df\n",
    "                                # Convert DataFrame to CSV\n",
    "                                csv_buffer = StringIO()\n",
    "                                df_download.to_csv(csv_buffer, index=False)\n",
    "                                csv_data = csv_buffer.getvalue()\n",
    "                                # Display download button\n",
    "                                st.download_button(\n",
    "                                    label=\"Download as CSV\",\n",
    "                                    data=csv_data,\n",
    "                                    file_name=\"active_data.csv\",\n",
    "                                    mime=\"text/csv\"\n",
    "                                )\n",
    "\n",
    "                                st.write(agent_else.run_sync(user_input).output)\n",
    "                                st.session_state.history.append((st.session_state.user_history, st.session_state.active_df.copy(), result.__str__()))\n",
    "\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error generating response: {str(e)}\")\n",
    "                    response = \"Sorry, I encountered an error while processing your request.\"\n",
    "\n",
    "    # Add a \"Clear Chat\" button to the sidebar\n",
    "    if st.sidebar.button('Clear Chat'):\n",
    "        # Clear chat history in db.json\n",
    "        # db = {'chat_history': []}\n",
    "        # with open(DB_FILE, 'w') as file:\n",
    "        #     json.dump(db, file)\n",
    "        # Clear chat messages in session state\n",
    "        st.session_state.user_history = \"\"\n",
    "        st.session_state.active_df = None\n",
    "        st.session_state.history = []\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "    st.sidebar.subheader(\"History\")\n",
    "    if st.session_state.history and st.session_state.user_history:\n",
    "        for idx, (instr, _, res) in enumerate(st.session_state.history[::-1], 1):\n",
    "            if len(instr) > 3: st.sidebar.markdown(f\"{idx}. **{instr}**\")\n",
    "\n",
    "def main():\n",
    "    app()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "import plotly.express as px\n",
    "from coinbase.feature_descriptions import feature_descriptions\n",
    "\n",
    "if spec is not None and spec.loader is not None:\n",
    "    cbds_fs_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(cbds_fs_module)\n",
    "    get = cbds_fs_module.get\n",
    "else:\n",
    "    raise ImportError(f\"Could not load module from {fs_module_path}\")\n",
    "\n",
    "def get_feature_output(feature_name: str, start_date: str, end_date: str):\n",
    "   \n",
    "    return get(start_date=start_date, end_date=end_date, features = [feature_name], dims = [], show_progress = False, inputs = {\"product_ids\": ['BTC-USD']})\n",
    "\n",
    "def aggregate_df(df, agg_func, feature_name):\n",
    "    # function to read the dataframe,\n",
    "    # ... see which columns to group by (all other than ds and the feature name itself)\n",
    "    # ... see whether the feature_name is numerical or categorical\n",
    "    # ... if numerical, then aggregate the feature_name using count.\n",
    "    # ... if categorical, then aggregate the feature_name using the agg_func\n",
    "    # ... return the aggregated dataframe\n",
    "    df = df.reset_index()\n",
    "    df_old = df.copy()\n",
    "\n",
    "    groupby_cols = [col for col in df.columns if col not in ['ds', feature_name, 'index']]\n",
    "    if groupby_cols:\n",
    "        if df[feature_name].dtype == 'string':\n",
    "            df = df.groupby(groupby_cols)[feature_name].count().reset_index()\n",
    "        else:\n",
    "            df = df.groupby(groupby_cols)[feature_name].agg(agg_func).reset_index()\n",
    "\n",
    "        df = df[[col for col in df.columns if col not in ['index']]].sort_values(by=feature_name, ascending=False)\n",
    "    else:\n",
    "        if 'ds' in df.columns: df = df[[col for col in df.columns if col not in ['index']]].sort_values(by='ds', ascending=False)\n",
    "    return df_old, df\n",
    "\n",
    "def groupby_aggregation_from_text(df, groupby_col, agg_col, agg_func):\n",
    "    \"\"\"\n",
    "    Groups the DataFrame based on a string instruction.\n",
    "   \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - groupby_col: column name to group by\n",
    "    - agg_col: column name to aggregate\n",
    "    - text: string to search for aggregation keywords\n",
    "\n",
    "    Returns:\n",
    "    - Grouped and aggregated DataFrame\n",
    "    \"\"\"\n",
    "    df_agg = df[agg_col].agg(agg_func)\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def format_large_number(num):\n",
    "    abs_num = abs(num)\n",
    "   \n",
    "    if abs_num >= 1_000_000_000:\n",
    "        return f\"{round(num / 1_000_000_000, 1)}B\"\n",
    "    elif abs_num >= 1_000_000:\n",
    "        return f\"{round(num / 1_000_000, 1)}M\"\n",
    "    elif abs_num >= 1_000:\n",
    "        return f\"{round(num / 1_000, 1)}K\"\n",
    "    else:\n",
    "        return str(num)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "from typing import Literal\n",
    "\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "class NodeAttributes(BaseModel):\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    granularity: Literal['daily', 'weekly', 'monthly', 'quarterly', 'yearly']\n",
    "    domain: str\n",
    "    hamilton_node: str\n",
    "    hamilton_description: str\n",
    "    aggregation: Literal['sum', 'mean', 'min', 'max', 'count', 'count_distinct', 'std', 'var', 'skew', 'kurt', 'mode', 'quantile', 'quantile_90', 'quantile_95', 'quantile_99']\n",
    "    source_file: str\n",
    "    node_match: bool\n",
    "\n",
    "def get_query_attributes(user_query_and_business_area: str, agent: Agent):\n",
    "    result = agent.run_sync(user_query_and_business_area)\n",
    "    return pd.Timestamp(result.output.start_date).strftime('%Y-%m-%d') , pd.Timestamp(result.output.end_date).strftime('%Y-%m-%d'), result.output.hamilton_node, result.output.business, result.output.domain, result.output.granularity, result.output.hamilton_description, result.output.source_file, result.output.node_match\n",
    "\n",
    "from typing import Literal, List, Union, Dict\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class DataFrameInfo(BaseModel):\n",
    "    \"\"\"Metadata about the DataFrame\"\"\"\n",
    "    shape: tuple[int, int]\n",
    "    columns: List[str]\n",
    "    dtypes: Dict[str, str]\n",
    "    sample_data: str\n",
    "    memory_usage: str\n",
    "    null_counts: Dict[str, int]\n",
    "\n",
    "\n",
    "class FilterRows(BaseModel):\n",
    "    function: Literal[\"filter_rows\"]\n",
    "    condition: str = Field(\n",
    "        description=\"\"\"filter condition to be used with pandas.DataFrame.query(). Always\n",
    "convert string values to lowercase, e.g df.query('name == \"charlie\"') -> `df.query(\"name.str.lower() == @search_term.lower())\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SortRows(BaseModel):\n",
    "    function: Literal[\"sort_rows\"]\n",
    "    by: str\n",
    "    ascending: bool = True\n",
    "\n",
    "\n",
    "class TopRows(BaseModel):\n",
    "    function: Literal[\"top_rows\"]\n",
    "    n: int\n",
    "\n",
    "\n",
    "class SelectColumns(BaseModel):\n",
    "    function: Literal[\"select_columns\"]\n",
    "    columns: List[str]\n",
    "\n",
    "\n",
    "class GroupBy(BaseModel):\n",
    "    function: Literal[\"group_by\"]\n",
    "    group_by_cols: List[str] = Field(\n",
    "        description=\"\"\"group by columns used for pandas.DataFrame.groupby(group_by_cols)[df.columns[-1]].agg(metric).\n",
    "IMPORTANT: Instead of `date`, use `ds`.\"\"\"\n",
    "    )\n",
    "    metric: Literal[\"first\", \"last\", \"mean\", \"median\", \"min\", \"max\", \"count\", \"count_distinct\", \"sum\", \"std\", \"var\", \"skew\", \"kurt\", \"mode\", \"quantile\", \"quantile_90\", \"quantile_95\", \"quantile_99\"] = Field(\n",
    "        description=\"\"\"group by metric used for pandas.DataFrame.groupby(group_by_cols)[df.columns[-1]].agg(metric).\n",
    "if unclear, use last.\"\"\"\n",
    "    )\n",
    "\n",
    "class RunDuckDBSQL(BaseModel):\n",
    "    function: Literal[\"run_duckdb_sql\"]\n",
    "    sql: str = Field(\n",
    "        description=\"\"\"\n",
    "Executes a SQL query against the in-memory dataframe using DuckDB syntax.\n",
    "\n",
    "Instructions for Formatting:\n",
    "- Put each WHERE clause on a new line\n",
    "- Use `ILIKE '%...%'` for string filters\n",
    "- Prefer CTEs for multi-step logic\n",
    "- Use 2-space indents for readability\n",
    "\n",
    "Instructions for Filtering and Aggregating:\n",
    "- If the user mentions the `date` column, use the `ds` column.\n",
    "\n",
    "Example:\n",
    "\n",
    "WITH filtered AS (\n",
    "  SELECT *\n",
    "  FROM df\n",
    "  WHERE\n",
    "    country ILIKE '%US%'\n",
    "    AND category ILIKE '%Retail%'\n",
    ")\n",
    "SELECT\n",
    "  date_trunc('month', ts)   AS month,\n",
    "  AVG(revenue)              AS avg_revenue\n",
    "FROM filtered\n",
    "GROUP BY month\n",
    "ORDER BY month\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "AggregationPlan = Union[FilterRows, SortRows, TopRows, SelectColumns, GroupBy, RunDuckDBSQL] # add get hamilton tool\n",
    "\n",
    "import json\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "SAFE_TOOLS = {}\n",
    "\n",
    "\n",
    "def register_tool(fn):\n",
    "    SAFE_TOOLS[fn.__name__] = fn\n",
    "    return fn\n",
    "\n",
    "\n",
    "@register_tool\n",
    "def filter_rows(df: pd.DataFrame, condition: str) -> pd.DataFrame:\n",
    "    SAFE_TOOLS['filter_rows'] = filter_rows\n",
    "    return df.query(condition)\n",
    "\n",
    "\n",
    "@register_tool\n",
    "def sort_rows(df: pd.DataFrame, by: str, ascending: bool = True) -> pd.DataFrame:\n",
    "    return df.sort_values(by=by, ascending=ascending)\n",
    "\n",
    "\n",
    "@register_tool\n",
    "def top_rows(df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    return df.head(n)\n",
    "\n",
    "\n",
    "def last_rows(df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    return df.tail(n)\n",
    "\n",
    "\n",
    "@register_tool\n",
    "def select_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    return df[columns]\n",
    "\n",
    "\n",
    "@register_tool\n",
    "def group_by(df: pd.DataFrame, group_by_cols: list, metric: str) -> pd.DataFrame:\n",
    "    df = df.groupby(group_by_cols)[df.columns[-1]].agg(metric)\n",
    "    df = df.reset_index()\n",
    "    # df = df.sort_values(by=list(df.columns), ascending=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "@register_tool\n",
    "def resample(df: pd.DataFrame, freq: str) -> pd.DataFrame:\n",
    "    return df.resample(freq).last()\n",
    "\n",
    "\n",
    "@register_tool\n",
    "def run_duckdb_sql(df: pd.DataFrame, sql: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run a SQL query on the in-memory DataFrame using DuckDB syntax.\n",
    "    You must use `df` as the table name.\n",
    "    \"\"\"\n",
    "    with duckdb.connect() as conn:\n",
    "        conn.register('df', df)\n",
    "        result_df = conn.execute(sql).fetchdf()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def create_dataframe_info(df: pd.DataFrame) -> DataFrameInfo:\n",
    "    \"\"\"Create comprehensive DataFrame metadata\"\"\"\n",
    "    return DataFrameInfo(\n",
    "        shape=df.shape,\n",
    "        columns=list(df.columns),\n",
    "        dtypes={str(col): str(dtype) for col, dtype in df.dtypes.items()},\n",
    "        sample_data=df.head(3).to_string(),\n",
    "        memory_usage=f\"{df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\",\n",
    "        null_counts={col: df[col].isnull().sum() for col in df.columns}\n",
    "    )\n",
    "\n",
    "def generate_system_prompt(df: pd.DataFrame = pd.DataFrame()):\n",
    "    df_info = create_dataframe_info(df)\n",
    "\n",
    "    # Dynamic system prompt with DataFrame context\n",
    "    system_prompt = f\"\"\"\n",
    "You're an expert data analyst assistant working with a DataFrame.\n",
    "\n",
    "DATASET OVERVIEW:\n",
    "- Shape: {df_info.shape[0]:,} rows by {df_info.shape[1]} columns\n",
    "- Columns: {', '.join(df_info.columns)}\n",
    "- Data Types: {json.dumps(df_info.dtypes, indent=2)}\n",
    "- Memory Usage: {df_info.memory_usage}\n",
    "- Missing Values: {json.dumps(df_info.null_counts, indent=2)}\n",
    "\n",
    "SAMPLE DATA:\n",
    "{df_info.sample_data}\n",
    "\n",
    "SCHEMA:\n",
    "{df.dtypes.to_dict()}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Always validate column names exist before operations\n",
    "2. Provide clear descriptions of what you're doing\n",
    "3. Handle errors gracefully\n",
    "4. Summarize results meaningfully\n",
    "5. Remember previous operations in the conversation\n",
    "\n",
    "You have access to various tools: If you can, always prefer to use of the `run_duckdb_sql` tool.\n",
    "\"\"\"\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "class QueryAction(BaseModel):\n",
    "    action: Literal['get_data', 'aggregate_data', 'graph_data', 'outlier_analysis']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def combinations(iterable, r):\n",
    "    for i in range(len(iterable)):\n",
    "        for j in range(i+1, len(iterable)):\n",
    "            yield (iterable[i], iterable[j])\n",
    "\n",
    "\n",
    "class ChartRecommendation(BaseModel):\n",
    "    \"\"\"\n",
    "    A complete recommendation for generating a single, insightful chart.\n",
    "    \"\"\"\n",
    "    chart_type: Literal[\"stacked_bar\", \"bar\", \"line_plot\", \"histogram\", \"pie_chart\", \"none\"] = Field(\n",
    "        ..., description=\"The type of chart to generate.\"\n",
    "    )\n",
    "    x_column: str = Field(None, description=\"The column for the x-axis.\")\n",
    "    y_column: str = Field(None, description=\"The column for the y-axis (numerical).\")\n",
    "    segment_column: str = Field(None, description=\"The column for color segmentation.\")\n",
    "    title: str = Field(..., description=\"A publication-quality title.\")\n",
    "    reasoning: str = Field(..., description=\"Explanation for why this chart was chosen.\")\n",
    "    color_palette_suggestion: Dict[str, str] = Field(None, description=\"Mapping of category names to hex color codes.\")\n",
    "    advice: str = Field(None, description=\"Helpful advice if no chart is plotted.\")\n",
    "\n",
    "def create_dataframe_info(df: pd.DataFrame) -> DataFrameInfo:\n",
    "    \"\"\"Create comprehensive DataFrame metadata\"\"\"\n",
    "    return DataFrameInfo(\n",
    "        shape=df.shape,\n",
    "        columns=list(df.columns),\n",
    "        dtypes={str(col): str(dtype) for col, dtype in df.dtypes.items()},\n",
    "        sample_data=df.head(3).to_string(),\n",
    "        memory_usage=f\"{df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\",\n",
    "        null_counts={col: df[col].isnull().sum() for col in df.columns}\n",
    "    )\n",
    "\n",
    "def _summarize_dataframe(df: pd.DataFrame) -> (str, bool):\n",
    "        \"\"\"\n",
    "        Creates a concise text summary of the DataFrame's columns,\n",
    "        ignoring columns with only one unique value.\n",
    "        \"\"\"\n",
    "        summary_lines = []\n",
    "        candidates_found = False\n",
    "       \n",
    "        for col in df.columns:\n",
    "            unique_count = df[col].nunique()\n",
    "           \n",
    "            # Edge Case: Ignore columns with only one value\n",
    "            if unique_count <= 1 and len(df) > 1:\n",
    "                continue\n",
    "\n",
    "            col_type = \"Unknown\"\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                col_type = \"Numerical\"\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                col_type = \"Temporal\"\n",
    "            elif df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
    "                col_type = \"Categorical\"\n",
    "           \n",
    "            samples = df[col].dropna().unique()[:3]\n",
    "            samples_str = \", \".join([str(s) for s in samples])\n",
    "           \n",
    "            summary_lines.append(\n",
    "                f\"- Column: '{col}', Type: {col_type}, Unique Values: {unique_count}, Samples: [{samples_str}]\"\n",
    "            )\n",
    "            candidates_found = True\n",
    "       \n",
    "        return \"\\n\".join(summary_lines), candidates_found\n",
    "\n",
    "def generate_graphing_system_prompt(df: pd.DataFrame = pd.DataFrame(), context: str = \"general business data\"):\n",
    "    \"\"\"\n",
    "        Analyzes the DataFrame and returns a complete chart recommendation.\n",
    "        \"\"\"\n",
    "\n",
    "    # Edge Case: Empty DataFrame\n",
    "    if df.empty:\n",
    "        return ChartRecommendation(\n",
    "            chart_type=ChartType.NONE,\n",
    "            x_column=None,\n",
    "            y_column=None,\n",
    "            segment_column=None,\n",
    "            title=\"No Data Found\",\n",
    "            reasoning=\"The provided DataFrame is empty.\",\n",
    "            advice=\"Please provide a DataFrame with data to generate a visualization.\",\n",
    "            color_palette_suggestion=None\n",
    "        )\n",
    "\n",
    "    summary, candidates_found = _summarize_dataframe(df)\n",
    "    print(summary)\n",
    "   \n",
    "    if not candidates_found:\n",
    "        return ChartRecommendation(\n",
    "            chart_type=ChartType.NONE,\n",
    "            x_column=None,\n",
    "            y_column=None,\n",
    "            segment_column=None,\n",
    "            title=\"No Suitable Data Found\",\n",
    "            reasoning=\"The DataFrame contains no columns suitable for plotting (e.g., all columns have only one unique value).\",\n",
    "            advice=\"Please check your data. A plottable column needs more than one unique value.\",\n",
    "            color_palette_suggestion=None\n",
    "        )\n",
    "\n",
    "    # Dynamic system prompt with DataFrame context\n",
    "    system_prompt = f\"\"\"\n",
    "        You are an expert data visualization specialist. Your task is to analyze a summary of a DataFrame and recommend the SINGLE BEST chart to represent the data, following a strict hierarchy of choices. You must provide all parameters needed to build the chart.\n",
    "\n",
    "        BUSINESS CONTEXT: The data represents {context}.\n",
    "\n",
    "        Follow this exact decision-making process in order of preference:\n",
    "\n",
    "        **1. If there are multiple categorical/temporal columns and at least one numerical column, make a STACKED BAR CHART. Prioritize the STACKED BAR CHART over the other chart types.**\n",
    "           - **Condition:** The DataFrame must contain at least one suitable categorical/temporal column.\n",
    "           - **Action:** If the condition is met, you must choose the first categorical/temporal column for the x-axis (`x_column`). Choose the other categorical/temporal column with the least number of unique values for segmentation (`segment_column`), EXCEPT if there is a column called 'symbol' or 'currency', in which case use that column for segmentation. The `y_column` should be the most relevant numerical column.\n",
    "           - If a stacked bar chart is chosen, suggest a professional color palette dictionary mapping the unique values of the `segment_column` to distinct hex codes.\n",
    "\n",
    "        **2. If there is one categorical column and one numerical column, try a BAR CHART.**\n",
    "           - **Condition:** The DataFrame must contain at least one suitable categorical/temporal column.\n",
    "           - **Action:** If the condition is met, you must choose the BEST categorical/temporal column for the x-axis (`x_column`) and another for segmentation (`segment_column`). The `y_column` should be the most relevant numerical column.\n",
    "\n",
    "        **3. If there is only one temporal column and one numerical column, try a LINE PLOT.**\n",
    "           - **Condition:** The DataFrame must contain ONE clear temporal column with many unique values (e.g., dates) AND ONE numerical column.\n",
    "           - **Action:** Set `x_column` to the temporal column and `y_column` to the numerical column.\n",
    "\n",
    "        **4. If neither of the above are suitable, consider a HISTOGRAM.**\n",
    "           - **Condition:** The DataFrame contains only numerical columns or no clear relationships between categorical and numerical data.\n",
    "           - **Action:** Choose the most important numerical column for the `x_column` to show its distribution. `y_column` will be 'Frequency'.\n",
    "\n",
    "        **5. Finally, if none of the above are possible, consider a PIE CHART.**\n",
    "           - **Condition:** The DataFrame has only ONE categorical column with a reasonable number of categories (2-10).\n",
    "           - **Action:** Set the `segment_column` to this categorical column.\n",
    "\n",
    "        **Edge Case:** If there is a temporal column but no numerical or categorical columns to plot against it, state this in your reasoning and provide advice.\n",
    "\n",
    "        Here is the summary of the available columns:\n",
    "        {summary}\n",
    "\n",
    "        Now, provide your final recommendation as a complete JSON object.\n",
    "        \"\"\"\n",
    "    return system_prompt\n",
    "\n",
    "def plot_with_agent_recommendation(df: pd.DataFrame, recommendation: ChartRecommendation):\n",
    "    if recommendation.chart_type == \"none\":\n",
    "        print(\"Plotting skipped.\")\n",
    "        if recommendation.advice: print(f\"Agent's Advice: {recommendation.advice}\")\n",
    "        return\n",
    "\n",
    "    fig = None\n",
    "    if recommendation.chart_type == \"stacked_bar\":\n",
    "        df = df.sort_values(by=[recommendation.x_column, recommendation.y_column], ascending=False)\n",
    "        # Get the top 5 values and group the rest as \"Other\"\n",
    "        top_5_values = list(df[recommendation.segment_column])[:5]\n",
    "        # top_5_values = value_counts.nlargest(5).index\n",
    "       \n",
    "        # Create a mask for values not in top 5\n",
    "        df_2 = df.copy()\n",
    "        df_2[recommendation.segment_column] = df_2[recommendation.segment_column].apply(\n",
    "            lambda x: x if x in top_5_values else \"Other\"\n",
    "        )\n",
    "        fig = px.bar(df_2, x=recommendation.x_column, y=recommendation.y_column, color=recommendation.segment_column, title=recommendation.title, color_discrete_map=recommendation.color_palette_suggestion)\n",
    "    elif recommendation.chart_type == \"bar\":\n",
    "        if recommendation.x_column == \"ds\": df = df.sort_values(by=recommendation.x_column)\n",
    "        fig = px.bar(df, x=recommendation.x_column, y=recommendation.y_column, title=recommendation.title, color_discrete_map=recommendation.color_palette_suggestion)\n",
    "    elif recommendation.chart_type == \"line_plot\":\n",
    "        if recommendation.x_column == \"ds\": df = df.sort_values(by=recommendation.x_column)\n",
    "        fig = px.line(df, x=recommendation.x_column, y=recommendation.y_column, title=recommendation.title, markers=True)\n",
    "    elif recommendation.chart_type == \"histogram\":\n",
    "        fig = px.histogram(df, x=recommendation.x_column, title=recommendation.title, marginal=\"box\")\n",
    "    elif recommendation.chart_type == \"pie_chart\":\n",
    "        fig = px.pie(df, names=recommendation.segment_column, title=recommendation.title, color_discrete_map=recommendation.color_palette_suggestion)\n",
    "   \n",
    "    if fig:\n",
    "        fig.update_layout(title_font_size=20, xaxis_title=recommendation.x_column.replace('_', ' ').title() if recommendation.x_column else None, yaxis_title=recommendation.y_column.replace('_', ' ').title() if recommendation.y_column else None, legend_title=recommendation.segment_column.replace('_', ' ').title() if recommendation.segment_column else None, font=dict(family=\"Arial, sans-serif\", size=12))\n",
    "        return fig\n",
    "    else:\n",
    "        print(\"Could not generate a chart for the given recommendation.\")\n",
    "\n",
    "router_prompt = \"\"\"You are an expert at comprehending the user's query and discerning the action they desire.\n",
    "        Take the user's query and understand whether they are asking for new data. If they are, return 'get data'.\n",
    "        If they are not (perhaps there are keywords like 'take this data' which means they want to use the data you've already provided).\n",
    "        Then understand whether they are asking for data to be aggregated, sorted, or filtered. If they are, return 'aggregate data'.\n",
    "        If they are not, take the user's query and understand whether they are asking for data to be graphed. If they are, return 'graph data'.\n",
    "        If they are not, take the user's query and understand whether they are asking for data trends to be analyzed. If they are, return 'outlier analysis'.\n",
    "        If none of these are what the user wants, return 'else', but be sure to try to understand if the user wants data first, and prefer to return 'get data' if possible.\n",
    "        You need to work quickly, so do not return more than one phrase\"\"\"\n",
    "\n",
    "get_prompt = f\"\"\"Parse the question and the business area and return a result in the output format.\n",
    "    For the output_fields `start_date` and `end_date`, please give start and end dates the user desires in 'YYYY-MM-DD' string format.\n",
    "    Please note that today's date is {today}.\n",
    "    For output field `business`, if the business name is not explicitly mentioned in the query, leave it as 'Unknown'.\n",
    "    For output field `aggregation`, if the aggregation function is not explicitly mentioned in the query, leave it as 'mean'.\n",
    "    For output field `granularity`, if the granularity is not explicitly mentioned in the query, leave it as 'daily'.\n",
    "    For output field `hamilton_node`, please search the following dictionary object for the `business_area` key. There should be a sub-dictionary for this `business_area`. Please exclusively search this sub-dictionary and return the key of the description from the sub-dictionary which is the best match. The dictionary object is here: {str(feature_descriptions)}.\n",
    "    For the output field `hamilton_description`, please return the value of the description of the hamilton node that is the best match.\n",
    "    For the output field `source_file`, please return the dictionary key of the source file that contains the feature.\n",
    "    For the output field `node_match`, please return True if the hamilton node found is in the sub-dictionary of the `business_area` key, and False otherwise.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
